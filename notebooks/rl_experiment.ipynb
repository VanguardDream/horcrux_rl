{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ray RLlib 강화학습 실험\n",
        "\n",
        "이 노트북은 Ray RLlib을 사용한 강화학습 연구의 기본 템플릿입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필수 라이브러리 import\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# 결과 저장 디렉토리 설정\n",
        "RESULTS_DIR = Path(\"../results\")\n",
        "RESULTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"라이브러리 import 완료\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Ray 초기화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ray 초기화\n",
        "# 로컬 모드로 실행 (디버깅용)\n",
        "# ray.init(local_mode=True)\n",
        "\n",
        "# 일반 모드로 실행 (실험용)\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "print(f\"Ray 클러스터 정보: {ray.cluster_resources()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 환경 설정\n",
        "\n",
        "**참고**: 실제 환경은 별도 프로젝트로 관리되므로, 여기서는 예시로 Gymnasium 환경을 사용합니다.\n",
        "실제 환경을 사용할 때는 해당 환경의 import 경로를 수정하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 환경 설정\n",
        "# TODO: 실제 환경으로 교체\n",
        "# from your_env_project import YourEnv\n",
        "\n",
        "# 예시: Gymnasium 환경 사용\n",
        "ENV_NAME = \"CartPole-v1\"  # 실제 환경으로 교체\n",
        "\n",
        "# 환경 정보 확인\n",
        "import gymnasium as gym\n",
        "env = gym.make(ENV_NAME)\n",
        "print(f\"환경: {ENV_NAME}\")\n",
        "print(f\"관찰 공간: {env.observation_space}\")\n",
        "print(f\"행동 공간: {env.action_space}\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. RLlib 알고리즘 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PPO 알고리즘 설정 (다른 알고리즘으로 변경 가능: DQN, A3C, SAC 등)\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(env=ENV_NAME)\n",
        "    .training(\n",
        "        lr=3e-4,\n",
        "        train_batch_size=4000,\n",
        "        sgd_minibatch_size=128,\n",
        "        num_sgd_iter=10,\n",
        "    )\n",
        "    .resources(num_gpus=0)  # GPU 사용 시 1로 변경\n",
        "    .rollouts(num_rollout_workers=2)  # 병렬 워커 수\n",
        "    .framework(\"torch\")  # \"tf2\" 또는 \"torch\"\n",
        ")\n",
        "\n",
        "print(\"알고리즘 설정 완료\")\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 알고리즘 인스턴스 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 알고리즘 인스턴스 생성\n",
        "algo = config.build()\n",
        "\n",
        "print(\"알고리즘 인스턴스 생성 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 실행\n",
        "NUM_ITERATIONS = 100  # 학습 반복 횟수\n",
        "training_history = []\n",
        "\n",
        "for i in range(NUM_ITERATIONS):\n",
        "    result = algo.train()\n",
        "    training_history.append({\n",
        "        'iteration': i + 1,\n",
        "        'episode_reward_mean': result.get('episode_reward_mean', 0),\n",
        "        'episode_len_mean': result.get('episode_len_mean', 0),\n",
        "        'policy_loss': result.get('info', {}).get('learner', {}).get('default_policy', {}).get('policy_loss', 0),\n",
        "    })\n",
        "    \n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Iteration {i + 1}: Mean Reward = {result.get('episode_reward_mean', 0):.2f}\")\n",
        "\n",
        "print(\"학습 완료\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 학습 결과 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 히스토리를 DataFrame으로 변환\n",
        "df = pd.DataFrame(training_history)\n",
        "\n",
        "# 결과 저장\n",
        "df.to_csv(RESULTS_DIR / \"training_history.csv\", index=False)\n",
        "print(f\"결과 저장 완료: {RESULTS_DIR / 'training_history.csv'}\")\n",
        "\n",
        "# 시각화\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# 에피소드 평균 보상\n",
        "axes[0].plot(df['iteration'], df['episode_reward_mean'])\n",
        "axes[0].set_xlabel('Iteration')\n",
        "axes[0].set_ylabel('Mean Episode Reward')\n",
        "axes[0].set_title('Training Progress: Mean Reward')\n",
        "axes[0].grid(True)\n",
        "\n",
        "# 에피소드 평균 길이\n",
        "axes[1].plot(df['iteration'], df['episode_len_mean'])\n",
        "axes[1].set_xlabel('Iteration')\n",
        "axes[1].set_ylabel('Mean Episode Length')\n",
        "axes[1].set_title('Training Progress: Mean Episode Length')\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / \"training_curves.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"시각화 저장 완료: {RESULTS_DIR / 'training_curves.png'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습된 정책으로 평가\n",
        "NUM_EVAL_EPISODES = 10\n",
        "eval_results = []\n",
        "\n",
        "env = gym.make(ENV_NAME, render_mode=None)\n",
        "\n",
        "for episode in range(NUM_EVAL_EPISODES):\n",
        "    obs, info = env.reset()\n",
        "    episode_reward = 0\n",
        "    episode_length = 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = algo.compute_single_action(obs)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        episode_reward += reward\n",
        "        episode_length += 1\n",
        "    \n",
        "    eval_results.append({\n",
        "        'episode': episode + 1,\n",
        "        'reward': episode_reward,\n",
        "        'length': episode_length\n",
        "    })\n",
        "\n",
        "env.close()\n",
        "\n",
        "eval_df = pd.DataFrame(eval_results)\n",
        "print(\"\\n평가 결과:\")\n",
        "print(eval_df)\n",
        "print(f\"\\n평균 보상: {eval_df['reward'].mean():.2f} ± {eval_df['reward'].std():.2f}\")\n",
        "print(f\"평균 에피소드 길이: {eval_df['length'].mean():.2f} ± {eval_df['length'].std():.2f}\")\n",
        "\n",
        "# 평가 결과 저장\n",
        "eval_df.to_csv(RESULTS_DIR / \"evaluation_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 모델 저장 및 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 저장\n",
        "checkpoint_dir = algo.save(RESULTS_DIR / \"checkpoint\")\n",
        "print(f\"모델 저장 완료: {checkpoint_dir}\")\n",
        "\n",
        "# 모델 로드 예시 (주석 처리)\n",
        "# loaded_algo = Algorithm.from_checkpoint(checkpoint_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 정리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 알고리즘 정리\n",
        "algo.stop()\n",
        "\n",
        "# Ray 종료 (필요한 경우)\n",
        "# ray.shutdown()\n",
        "\n",
        "print(\"정리 완료\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gd",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
