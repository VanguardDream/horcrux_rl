{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib 강화학습 실험\n",
    "\n",
    "이 노트북은 Ray RLlib을 사용한 강화학습 연구의 기본 템플릿입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 import\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.connectors.env_to_module import EnvToModulePipeline\n",
    "from ray.rllib.connectors.module_to_env import ModuleToEnvPipeline\n",
    "from ray.rllib.core.columns import Columns\n",
    "from ray.rllib.env.single_agent_episode import SingleAgentEpisode\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 결과 저장 디렉토리 설정\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"라이브러리 import 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ray 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray 초기화\n",
    "# 로컬 모드로 실행 (디버깅용)\n",
    "# ray.init(local_mode=True)\n",
    "\n",
    "# 일반 모드로 실행 (실험용)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "print(f\"Ray 클러스터 정보: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 설정\n",
    "\n",
    "**참고**: 실제 환경은 별도 프로젝트로 관리되므로, 여기서는 예시로 Gymnasium 환경을 사용합니다.\n",
    "실제 환경을 사용할 때는 해당 환경의 import 경로를 수정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"horcrux_env/plane-v0\"\n",
    "\n",
    "# 환경 정보 확인\n",
    "import gymnasium as gym\n",
    "import horcrux_env\n",
    "from horcrux_env.envs import PlaneJoyDirWorld\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "print(f\"환경: {ENV_NAME}\")\n",
    "print(f\"관찰 공간: {env.observation_space}\")\n",
    "print(f\"행동 공간: {env.action_space}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_CONFIG = {\n",
    "    \"forward_reward_weight\": 175.0,\n",
    "    \"rotation_reward_weight\": 100.0,\n",
    "    \"unhealthy_max_steps\": 150.0,\n",
    "    \"healthy_reward\": 3.0,\n",
    "    \"healthy_roll_range\": (-40,40),\n",
    "    \"terminating_roll_range\": (-85,85),\n",
    "    \"rotation_norm_cost_weight\": 14.5,\n",
    "    \"termination_reward\": 0,\n",
    "    \"gait_params\": (30, 30, 40, 40, 0, -1),\n",
    "    \"use_friction_chg\": True,\n",
    "    \"joy_input_random\": True,\n",
    "    \"use_imu_window\": True,\n",
    "    \"use_vels_window\": True,\n",
    "    \"ctrl_cost_weight\": 0.05,\n",
    "}\n",
    "\n",
    "RENDER_ENV_CONFIG = ENV_CONFIG.copy()\n",
    "RENDER_ENV_CONFIG['render_mode'] = 'rgb_array'\n",
    "RENDER_ENV_CONFIG['render_camera_name'] = 'ceiling'\n",
    "\n",
    "# env = gym.make(ENV_NAME, **RENDER_ENV_CONFIG)\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(ENV_NAME, lambda config: PlaneJoyDirWorld( forward_reward_weight=ENV_CONFIG[\"forward_reward_weight\"], \n",
    "                                                     rotation_reward_weight=ENV_CONFIG[\"rotation_reward_weight\"], \n",
    "                                                     unhealthy_max_steps=ENV_CONFIG[\"unhealthy_max_steps\"],\n",
    "                                                     healthy_reward=ENV_CONFIG[\"healthy_reward\"], \n",
    "                                                     healthy_roll_range=ENV_CONFIG[\"healthy_roll_range\"],\n",
    "                                                     terminating_roll_range=ENV_CONFIG[\"terminating_roll_range\"],\n",
    "                                                     rotation_norm_cost_weight=ENV_CONFIG[\"rotation_norm_cost_weight\"],\n",
    "                                                     termination_reward=ENV_CONFIG[\"termination_reward\"],\n",
    "                                                     gait_params=ENV_CONFIG[\"gait_params\"],\n",
    "                                                     use_friction_chg=ENV_CONFIG[\"use_friction_chg\"],\n",
    "                                                     joy_input_random=ENV_CONFIG[\"joy_input_random\"],\n",
    "                                                     use_imu_window=ENV_CONFIG[\"use_imu_window\"],\n",
    "                                                     ctrl_cost_weight=ENV_CONFIG[\"ctrl_cost_weight\"],\n",
    "                                                   )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RLlib 알고리즘 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 알고리즘 설정 (다른 알고리즘으로 변경 가능: DQN, A3C, SAC 등)\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=ENV_NAME)\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        train_batch_size=65536,\n",
    "        minibatch_size=16384,\n",
    "        gamma=0.95,\n",
    "        num_sgd_iter=60,\n",
    "    )\n",
    "    .resources(num_gpus=1)  # GPU 사용 시 1로 변경\n",
    "    .framework(\"torch\")  # \"tf2\" 또는 \"torch\"\n",
    "    .env_runners(\n",
    "        num_env_runners=16,\n",
    "        num_envs_per_env_runner=4,\n",
    "    )\n",
    "    .learners(\n",
    "        num_learners=1,\n",
    "        num_gpus_per_learner=1,\n",
    "    )\n",
    "    .rl_module(\n",
    "        model_config={\n",
    "            \"fcnet_hiddens\": [512, 512, 512, 512, 512, 64],\n",
    "            \"vf_share_layers\": False,\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"알고리즘 설정 완료\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 알고리즘 인스턴스 생성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알고리즘 인스턴스 생성\n",
    "algo = config.build_algo()\n",
    "\n",
    "print(\"알고리즘 인스턴스 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실행\n",
    "NUM_ITERATIONS = 100  # 학습 반복 횟수\n",
    "\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    result = algo.train()\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Iteration {i + 1}\")\n",
    "\n",
    "print(\"학습 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 히스토리를 DataFrame으로 변환\n",
    "df = pd.DataFrame(training_history)\n",
    "\n",
    "# 결과 저장\n",
    "df.to_csv(RESULTS_DIR / \"training_history.csv\", index=False)\n",
    "print(f\"결과 저장 완료: {RESULTS_DIR / 'training_history.csv'}\")\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 에피소드 평균 보상\n",
    "axes[0].plot(df['iteration'], df['episode_reward_mean'])\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Mean Episode Reward')\n",
    "axes[0].set_title('Training Progress: Mean Reward')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# 에피소드 평균 길이\n",
    "axes[1].plot(df['iteration'], df['episode_len_mean'])\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Mean Episode Length')\n",
    "axes[1].set_title('Training Progress: Mean Episode Length')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"시각화 저장 완료: {RESULTS_DIR / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.connectors.env_to_module import EnvToModulePipeline\n",
    "from ray.rllib.connectors.module_to_env import ModuleToEnvPipeline\n",
    "\n",
    "# 학습된 정책으로 평가 (새 API 스택 사용)\n",
    "NUM_EVAL_EPISODES = 10\n",
    "eval_results = []\n",
    "\n",
    "# RLModule과 Connector 파이프라인 가져오기\n",
    "rl_module = algo.get_module()\n",
    "\n",
    "# EnvToModule과 ModuleToEnv 파이프라인 가져오기\n",
    "# algo 객체에서 직접 가져올 수 있는 경우\n",
    "try:\n",
    "    # env_runner에서 connector 가져오기\n",
    "    env_runner = algo.env_runner\n",
    "    env_to_module = env_runner._connectors[0] if hasattr(env_runner, '_connectors') else None\n",
    "    module_to_env = env_runner._connectors[1] if hasattr(env_runner, '_connectors') and len(env_runner._connectors) > 1 else None\n",
    "except:\n",
    "    env_to_module = None\n",
    "    module_to_env = None\n",
    "\n",
    "# 파이프라인을 직접 생성해야 하는 경우\n",
    "if env_to_module is None or module_to_env is None:\n",
    "    # 간단한 방법: RLModule만 사용하고 직접 처리\n",
    "    from ray.rllib.core.columns import Columns\n",
    "    \n",
    "    env = gym.make(ENV_NAME, render_mode=None)\n",
    "    device = getattr(rl_module, \"device\", next(rl_module.parameters()).device)\n",
    "    \n",
    "    for episode in range(NUM_EVAL_EPISODES):\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        \n",
    "        # Episode 객체 생성\n",
    "        episode_obj = SingleAgentEpisode(\n",
    "            observations=[obs],\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "        )\n",
    "        \n",
    "        # Option 1\n",
    "        while not done:\n",
    "            # 관찰을 텐서로 변환\n",
    "            obs_batch = np.expand_dims(obs, 0).astype(np.float32)\n",
    "            obs_tensor = torch.from_numpy(obs_batch).to(device)\n",
    "            \n",
    "            # forward_inference 사용 (평가 시에는 탐색 없음)\n",
    "            rl_module_out = rl_module.forward_inference({\"obs\": obs_tensor})\n",
    "            \n",
    "            # ModuleToEnv 파이프라인이 없으므로 직접 처리\n",
    "            # action_dist_inputs에서 액션 추출\n",
    "            if Columns.ACTION_DIST_INPUTS in rl_module_out:\n",
    "                action_dist_inputs = rl_module_out[Columns.ACTION_DIST_INPUTS]\n",
    "            elif \"action_dist_inputs\" in rl_module_out:\n",
    "                action_dist_inputs = rl_module_out[\"action_dist_inputs\"]\n",
    "            else:\n",
    "                # 다른 키 확인\n",
    "                action_dist_inputs = list(rl_module_out.values())[0]\n",
    "            \n",
    "            # action_dist_inputs 처리 (mean만 사용하거나 분포에서 샘플링)\n",
    "            if isinstance(action_dist_inputs, torch.Tensor):\n",
    "                dist_params = action_dist_inputs[0] if len(action_dist_inputs.shape) > 1 else action_dist_inputs\n",
    "                \n",
    "                if len(dist_params.shape) == 0 or dist_params.shape[0] == 14:\n",
    "                    # mean만 있는 경우 (deterministic)\n",
    "                    action_tensor = dist_params[:14] if len(dist_params) >= 14 else dist_params\n",
    "                elif dist_params.shape[0] == 28:\n",
    "                    # mean(14) + log_std(14) 구조\n",
    "                    mean = dist_params[:14]\n",
    "                    log_std = dist_params[14:]\n",
    "                    std = torch.exp(log_std.clamp(-20, 2))\n",
    "                    from torch.distributions import Normal\n",
    "                    normal_dist = Normal(mean, std)\n",
    "                    action_tensor = normal_dist.sample()\n",
    "                else:\n",
    "                    action_tensor = dist_params\n",
    "                \n",
    "                # 액션 공간에 맞게 클리핑\n",
    "                action_low = torch.tensor(env.action_space.low, device=device, dtype=action_tensor.dtype)\n",
    "                action_high = torch.tensor(env.action_space.high, device=device, dtype=action_tensor.dtype)\n",
    "                \n",
    "                \n",
    "                bounded_action = torch.tanh(action_tensor[:14])\n",
    "                # print(f'over:{(bounded_action[0:14] > 1).sum()}, below:{(bounded_action[0:14] < -1).sum()}, total:{(bounded_action[0:14] > 1).sum() + (bounded_action[0:14] < -1).sum()}')\n",
    "                \n",
    "                real_action = action_low + (0.5 * (bounded_action + 1.0) * (action_high - action_low))\n",
    "                # print(real_action)\n",
    "\n",
    "                action = real_action.detach().cpu().numpy()\n",
    "            else:\n",
    "                action = np.array(action_dist_inputs)\n",
    "                if len(action.shape) > 1:\n",
    "                    action = action[0]\n",
    "                action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            if episode_length > 6000:\n",
    "                done = True\n",
    "\n",
    "            # Episode 업데이트\n",
    "            episode_obj.add_env_step(\n",
    "                obs,\n",
    "                action,\n",
    "                reward,\n",
    "                terminated=terminated,\n",
    "                truncated=truncated,\n",
    "            )\n",
    "        \n",
    "        # Option 2\n",
    "        # while not done:\n",
    "        #     input_dict = env_to_module(\n",
    "        #         episodes=[episode_obj],\n",
    "        #         rl_module=rl_module,\n",
    "        #         explore=False,\n",
    "        #         shared_data={},\n",
    "        #     )\n",
    "\n",
    "        #     rl_module_out = rl_module.forward_inference(input_dict)\n",
    "\n",
    "        #     to_env = module_to_env(\n",
    "        #         batch=rl_module_out,\n",
    "        #         episodes=[episode_obj],\n",
    "        #         rl_module=rl_module,\n",
    "        #         explore=False,\n",
    "        #         shared_data={},\n",
    "        #     )\n",
    "\n",
    "        #     action = to_env.pop(Columns.ACTIONS)[0]\n",
    "            \n",
    "        #     obs, reward, terminated, truncated, info = env.step(action)\n",
    "        #     done = terminated or truncated\n",
    "        #     episode_reward += reward\n",
    "        #     episode_length += 1\n",
    "            \n",
    "        #     # Episode 업데이트\n",
    "        #     episode_obj.add_env_step(\n",
    "        #         obs,\n",
    "        #         action,\n",
    "        #         reward,\n",
    "        #         terminated=terminated,\n",
    "        #         truncated=truncated,\n",
    "        #     )\n",
    "            \n",
    "\n",
    "        print(episode_obj.get_return())\n",
    "        eval_results.append({\n",
    "            'episode': episode + 1,\n",
    "            'reward': episode_reward,\n",
    "            'length': episode_length,\n",
    "        })\n",
    "        \n",
    "        if episode_obj.is_done:\n",
    "            obs, info = env.reset()\n",
    "            episode_obj = SingleAgentEpisode(\n",
    "                observations=[obs],\n",
    "                observation_space=env.observation_space,\n",
    "                action_space=env.action_space,\n",
    "            )\n",
    "    \n",
    "    env.close()\n",
    "else:\n",
    "    # 파이프라인이 있는 경우 예제 방식 사용\n",
    "    env = gym.make(ENV_NAME, render_mode=None)\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    episode_obj = SingleAgentEpisode(\n",
    "        observations=[obs],\n",
    "        observation_space=env.observation_space,\n",
    "        action_space=env.action_space,\n",
    "    )\n",
    "    \n",
    "    num_episodes = 0\n",
    "    while num_episodes < NUM_EVAL_EPISODES:\n",
    "        shared_data = {}\n",
    "        input_dict = env_to_module(\n",
    "            episodes=[episode_obj],\n",
    "            rl_module=rl_module,\n",
    "            explore=False,\n",
    "            shared_data=shared_data,\n",
    "        )\n",
    "        \n",
    "        rl_module_out = rl_module.forward_inference(input_dict)\n",
    "        \n",
    "        to_env = module_to_env(\n",
    "            batch=rl_module_out,\n",
    "            episodes=[episode_obj],\n",
    "            rl_module=rl_module,\n",
    "            explore=False,\n",
    "            shared_data=shared_data,\n",
    "        )\n",
    "        \n",
    "        action = to_env.pop(Columns.ACTIONS)[0]\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        episode_obj.add_env_step(\n",
    "            obs,\n",
    "            action,\n",
    "            reward,\n",
    "            terminated=terminated,\n",
    "            truncated=truncated,\n",
    "            extra_model_outputs={k: v[0] for k, v in to_env.items()},\n",
    "        )\n",
    "        \n",
    "        if episode_obj.is_done:\n",
    "            eval_results.append({\n",
    "                'episode': num_episodes + 1,\n",
    "                'reward': episode_obj.get_return(),\n",
    "                'length': len(episode_obj),\n",
    "            })\n",
    "            obs, info = env.reset()\n",
    "            episode_obj = SingleAgentEpisode(\n",
    "                observations=[obs],\n",
    "                observation_space=env.observation_space,\n",
    "                action_space=env.action_space,\n",
    "            )\n",
    "            num_episodes += 1\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "\n",
    "eval_df = pd.DataFrame(eval_results)\n",
    "print(\"\\n평가 결과:\")\n",
    "print(eval_df)\n",
    "print(f\"\\n평균 보상: {eval_df['reward'].mean():.2f} ± {eval_df['reward'].std():.2f}\")\n",
    "print(f\"평균 에피소드 길이: {eval_df['length'].mean():.2f} ± {eval_df['length'].std():.2f}\")\n",
    "\n",
    "# # 평가 결과 저장\n",
    "# eval_df.to_csv(RESULTS_DIR / \"evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 저장 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.save(algo.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "checkpoint_dir = algo.save()\n",
    "print(f\"모델 저장 완료: {checkpoint_dir}\")\n",
    "\n",
    "# 모델 로드 예시 (주석 처리)\n",
    "# loaded_algo = Algorithm.from_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알고리즘 정리\n",
    "algo.stop()\n",
    "\n",
    "# Ray 종료 (필요한 경우)\n",
    "# ray.shutdown()\n",
    "\n",
    "print(\"정리 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
